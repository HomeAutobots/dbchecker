#!/usr/bin/env python3
"""
Test Report Viewer for dbchecker.

This script provides utilities to view, compare, and analyze test reports
generated by the dbchecker test suite.
"""

import json
import sys
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import argparse

def load_report(report_path: Path) -> Optional[Dict]:
    """Load a test report from JSON file."""
    try:
        with open(report_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading report {report_path}: {e}")
        return None

def list_available_reports(reports_dir: Path) -> List[Path]:
    """List all available test reports."""
    if not reports_dir.exists():
        return []
    
    # Look for both our custom reports and pytest reports
    patterns = [
        "test_report_*.json",
        "pytest_report_*.json", 
        "test_summary_*.json"
    ]
    
    reports = []
    for pattern in patterns:
        reports.extend(reports_dir.glob(pattern))
    
    return sorted(reports, key=lambda x: x.stat().st_mtime, reverse=True)

def display_report_summary(report: Dict, report_path: Path):
    """Display a summary of a test report."""
    print(f"\nReport: {report_path.name}")
    print("=" * 60)
    
    # Try to extract basic info
    timestamp = report.get('timestamp') or report.get('report_metadata', {}).get('timestamp')
    if timestamp:
        try:
            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            print(f"Generated: {dt.strftime('%Y-%m-%d %H:%M:%S')}")
        except:
            print(f"Generated: {timestamp}")
    
    # Handle different report formats
    if 'summary' in report:
        # Our custom report format
        summary = report['summary']
        print(f"Total Tests: {summary.get('total_tests', 'N/A')}")
        print(f"Passed: {summary.get('passed_tests', 'N/A')}")
        print(f"Failed: {summary.get('failed_tests', 'N/A')}")
        print(f"Errors: {summary.get('error_tests', 'N/A')}")
        print(f"Skipped: {summary.get('skipped_tests', 'N/A')}")
        print(f"Success Rate: {summary.get('success_rate', 'N/A')}%")
        print(f"Coverage: {summary.get('coverage_percentage', 'N/A')}%")
        
        if 'total_duration' in report.get('report_metadata', {}):
            print(f"Duration: {report['report_metadata']['total_duration']}s")
    
    elif 'statistics' in report:
        # Pytest summary format
        stats = report['statistics']
        print(f"Total Tests: {stats.get('total', 'N/A')}")
        print(f"Passed: {stats.get('passed', 'N/A')}")
        print(f"Failed: {stats.get('failed', 'N/A')}")
        print(f"Errors: {stats.get('error', 'N/A')}")
        print(f"Skipped: {stats.get('skipped', 'N/A')}")
        
        if stats.get('total', 0) > 0:
            success_rate = (stats.get('passed', 0) / stats['total']) * 100
            print(f"Success Rate: {success_rate:.1f}%")
        
        if 'coverage' in report:
            cov = report['coverage']
            print(f"Coverage: {cov.get('percent_covered', 'N/A')}%")
        
        if 'duration' in report:
            print(f"Duration: {report['duration']:.2f}s")
    
    elif 'summary' in report and 'total' in report['summary']:
        # Direct pytest JSON format
        summary = report['summary']
        print(f"Total Tests: {summary.get('total', 'N/A')}")
        print(f"Passed: {summary.get('passed', 'N/A')}")
        print(f"Failed: {summary.get('failed', 'N/A')}")
        print(f"Errors: {summary.get('error', 'N/A')}")
        print(f"Skipped: {summary.get('skipped', 'N/A')}")
        
        if summary.get('total', 0) > 0:
            success_rate = (summary.get('passed', 0) / summary['total']) * 100
            print(f"Success Rate: {success_rate:.1f}%")
        
        if 'duration' in report:
            print(f"Duration: {report['duration']:.2f}s")

def compare_reports(report1: Dict, report2: Dict, name1: str, name2: str):
    """Compare two test reports and show differences."""
    print(f"\nComparing {name1} vs {name2}")
    print("=" * 60)
    
    def extract_stats(report):
        """Extract statistics from different report formats."""
        stats = {}
        
        if 'summary' in report and 'total_tests' in report['summary']:
            # Our custom format
            summary = report['summary']
            stats = {
                'total': summary.get('total_tests', 0),
                'passed': summary.get('passed_tests', 0),
                'failed': summary.get('failed_tests', 0),
                'errors': summary.get('error_tests', 0),
                'skipped': summary.get('skipped_tests', 0),
                'success_rate': summary.get('success_rate', 0),
                'coverage': summary.get('coverage_percentage', 0),
                'duration': report.get('report_metadata', {}).get('total_duration', 0)
            }
        elif 'statistics' in report:
            # Pytest summary format
            st = report['statistics']
            stats = {
                'total': st.get('total', 0),
                'passed': st.get('passed', 0),
                'failed': st.get('failed', 0),
                'errors': st.get('error', 0),
                'skipped': st.get('skipped', 0),
                'success_rate': (st.get('passed', 0) / st.get('total', 1)) * 100 if st.get('total', 0) > 0 else 0,
                'coverage': report.get('coverage', {}).get('percent_covered', 0),
                'duration': report.get('duration', 0)
            }
        elif 'summary' in report and 'total' in report['summary']:
            # Direct pytest format
            summary = report['summary']
            stats = {
                'total': summary.get('total', 0),
                'passed': summary.get('passed', 0),
                'failed': summary.get('failed', 0),
                'errors': summary.get('error', 0),
                'skipped': summary.get('skipped', 0),
                'success_rate': (summary.get('passed', 0) / summary.get('total', 1)) * 100 if summary.get('total', 0) > 0 else 0,
                'coverage': 0,  # Not available in basic pytest format
                'duration': report.get('duration', 0)
            }
        
        return stats
    
    stats1 = extract_stats(report1)
    stats2 = extract_stats(report2)
    
    metrics = [
        ('Total Tests', 'total'),
        ('Passed', 'passed'),
        ('Failed', 'failed'),
        ('Errors', 'errors'),
        ('Skipped', 'skipped'),
        ('Success Rate (%)', 'success_rate'),
        ('Coverage (%)', 'coverage'),
        ('Duration (s)', 'duration')
    ]
    
    print(f"{'Metric':<20} {'Report 1':<15} {'Report 2':<15} {'Change':<15}")
    print("-" * 65)
    
    for metric_name, key in metrics:
        val1 = stats1.get(key, 0)
        val2 = stats2.get(key, 0)
        
        if isinstance(val1, float) or isinstance(val2, float):
            change = val2 - val1
            change_str = f"{change:+.2f}"
            val1_str = f"{val1:.2f}"
            val2_str = f"{val2:.2f}"
        else:
            change = val2 - val1
            change_str = f"{change:+d}" if change != 0 else "0"
            val1_str = str(val1)
            val2_str = str(val2)
        
        # Color coding for change
        if change > 0 and key in ['total', 'passed', 'success_rate', 'coverage']:
            change_indicator = "📈"
        elif change < 0 and key in ['failed', 'errors', 'duration']:
            change_indicator = "📈"
        elif change > 0 and key in ['failed', 'errors', 'duration']:
            change_indicator = "📉"
        elif change < 0 and key in ['total', 'passed', 'success_rate', 'coverage']:
            change_indicator = "📉"
        else:
            change_indicator = "➡️"
        
        print(f"{metric_name:<20} {val1_str:<15} {val2_str:<15} {change_str:<10} {change_indicator}")

def show_detailed_failures(report: Dict, report_name: str):
    """Show detailed failure information from a report."""
    print(f"\nDetailed Failures for {report_name}")
    print("=" * 60)
    
    failures = []
    errors = []
    
    # Extract failures based on report format
    if 'test_results' in report:
        # Our custom format
        test_results = report['test_results']
        failures = test_results.get('failures', [])
        errors = test_results.get('errors', [])
    elif 'tests' in report:
        # Pytest format
        for test in report['tests']:
            if test.get('outcome') == 'failed':
                failures.append({
                    'test_class': test.get('nodeid', '').split('::')[0] if '::' in test.get('nodeid', '') else 'Unknown',
                    'test_method': test.get('nodeid', '').split('::')[-1] if '::' in test.get('nodeid', '') else test.get('nodeid', 'Unknown'),
                    'error': test.get('call', {}).get('longrepr', 'No error details available')
                })
            elif test.get('outcome') == 'error':
                errors.append({
                    'test_class': test.get('nodeid', '').split('::')[0] if '::' in test.get('nodeid', '') else 'Unknown',
                    'test_method': test.get('nodeid', '').split('::')[-1] if '::' in test.get('nodeid', '') else test.get('nodeid', 'Unknown'),
                    'error': test.get('setup', {}).get('longrepr') or test.get('call', {}).get('longrepr', 'No error details available')
                })
    
    if failures:
        print(f"\nFAILURES ({len(failures)}):")
        print("-" * 40)
        for i, failure in enumerate(failures, 1):
            print(f"{i}. {failure.get('test_class', 'Unknown')}.{failure.get('test_method', 'Unknown')}")
            error_text = failure.get('error', 'No details available')
            # Truncate very long error messages
            if len(error_text) > 500:
                error_text = error_text[:500] + "... (truncated)"
            print(f"   Error: {error_text}")
            print()
    
    if errors:
        print(f"\nERRORS ({len(errors)}):")
        print("-" * 40)
        for i, error in enumerate(errors, 1):
            print(f"{i}. {error.get('test_class', 'Unknown')}.{error.get('test_method', 'Unknown')}")
            error_text = error.get('error', 'No details available')
            # Truncate very long error messages
            if len(error_text) > 500:
                error_text = error_text[:500] + "... (truncated)"
            print(f"   Error: {error_text}")
            print()
    
    if not failures and not errors:
        print("No failures or errors found in this report! 🎉")

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='View and analyze dbchecker test reports')
    parser.add_argument('command', choices=['list', 'show', 'compare', 'failures'],
                       help='Command to execute')
    parser.add_argument('--reports-dir', type=Path, 
                       default=Path.cwd() / 'test_reports',
                       help='Directory containing test reports')
    parser.add_argument('--report1', type=str,
                       help='First report file (for compare) or report to show')
    parser.add_argument('--report2', type=str,
                       help='Second report file (for compare)')
    parser.add_argument('--latest', action='store_true',
                       help='Use the latest report')
    
    args = parser.parse_args()
    
    reports_dir = args.reports_dir
    
    if args.command == 'list':
        reports = list_available_reports(reports_dir)
        if not reports:
            print("No test reports found.")
            return
        
        print("Available Test Reports:")
        print("=" * 40)
        for i, report_path in enumerate(reports, 1):
            stat = report_path.stat()
            mod_time = datetime.fromtimestamp(stat.st_mtime)
            size = stat.st_size
            print(f"{i:2d}. {report_path.name}")
            print(f"    Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"    Size: {size:,} bytes")
            print()
    
    elif args.command == 'show':
        if args.latest:
            # Look for latest report
            latest_path = reports_dir / "latest_test_summary.json"
            if not latest_path.exists():
                latest_path = reports_dir / "latest_report.json"
            
            if not latest_path.exists():
                reports = list_available_reports(reports_dir)
                if reports:
                    latest_path = reports[0]  # Most recent
                else:
                    print("No reports found.")
                    return
                    
            report_path = latest_path
        elif args.report1:
            report_path = reports_dir / args.report1
            if not report_path.exists():
                # Try to find it by partial name
                matches = list(reports_dir.glob(f"*{args.report1}*"))
                if matches:
                    report_path = matches[0]
                else:
                    print(f"Report not found: {args.report1}")
                    return
        else:
            print("Please specify --report1 or --latest")
            return
        
        report = load_report(report_path)
        if report:
            display_report_summary(report, report_path)
    
    elif args.command == 'compare':
        if not args.report1 or not args.report2:
            print("Please specify both --report1 and --report2 for comparison")
            return
        
        report1_path = reports_dir / args.report1
        report2_path = reports_dir / args.report2
        
        # Try partial name matching
        if not report1_path.exists():
            matches = list(reports_dir.glob(f"*{args.report1}*"))
            if matches:
                report1_path = matches[0]
        
        if not report2_path.exists():
            matches = list(reports_dir.glob(f"*{args.report2}*"))
            if matches:
                report2_path = matches[0]
        
        report1 = load_report(report1_path)
        report2 = load_report(report2_path)
        
        if report1 and report2:
            compare_reports(report1, report2, report1_path.name, report2_path.name)
        else:
            print("Failed to load one or both reports")
    
    elif args.command == 'failures':
        if args.latest:
            # Look for latest report
            latest_path = reports_dir / "latest_test_summary.json"
            if not latest_path.exists():
                latest_path = reports_dir / "latest_report.json"
            
            if not latest_path.exists():
                reports = list_available_reports(reports_dir)
                if reports:
                    latest_path = reports[0]  # Most recent
                else:
                    print("No reports found.")
                    return
                    
            report_path = latest_path
        elif args.report1:
            report_path = reports_dir / args.report1
            if not report_path.exists():
                # Try to find it by partial name
                matches = list(reports_dir.glob(f"*{args.report1}*"))
                if matches:
                    report_path = matches[0]
                else:
                    print(f"Report not found: {args.report1}")
                    return
        else:
            print("Please specify --report1 or --latest")
            return
        
        report = load_report(report_path)
        if report:
            show_detailed_failures(report, report_path.name)

if __name__ == '__main__':
    main()
